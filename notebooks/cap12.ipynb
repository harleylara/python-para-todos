{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--BOOK_INFORMATION-->\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"./figures/cover-small.jpg\">\n",
    "\n",
    "*Este libro es una versión al español de [Python for Everybody](https://www.py4e.com/) escrito por el [Dr. Charles R. Severance](http://www.dr-chuck.com/); este contenido esta disponible en [GitHub](https://github.com/csev/py4e).*\n",
    "\n",
    "Detalles de Copyright\n",
    "\n",
    "*Copyright ~ 2009- Charles Severance.\n",
    "Este trabajo está registrado bajo una Licencia Creative Commons AttributionNonCommercial-ShareAlike 3.0 [CC BY-NC-SA](https://creativecommons.org/licenses/by-nc-sa/3.0/).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "| [Indice](indice.ipynb) | \n",
    "\n",
    "< [Capítulo 11 - Expresiones regulares](cap11.ipynb) | [Capítulo 13 - Python y servicios web](cap13.ipynb) >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capítulo 12 - Programas en red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien muchos de los ejemplos de este libro se han centrado en leer archivos y buscar datos en esos archivos, hay muchas fuentes diferentes de información cuando uno considera Internet.\n",
    "\n",
    "En este capítulo pretendemos ser un navegador web y recuperar páginas web utilizando el protocolo de transferencia de hipertexto (HTTP). Luego leeremos los datos de la página web y lo analizaremos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protocolo de transferencia de hipertexto – HTTP\n",
    "\n",
    "El protocolo de red que alimenta la web es en realidad bastante simple y hay soporte integrado en Python llamado, sockets que hace que sea muy fácil hacer conexiones de red y recuperar datos sobre esos sockets en un programa de Python.\n",
    "\n",
    "Un socket es muy similar a un archivo, excepto que un solo socket proporciona una conexión bidireccional entre dos programas. Puede leer y escribir en el mismo socket. Si escribe algo en un socket, se envía a la aplicación en el otro extremo del socket. Si lee desde el socket, se le proporcionan los datos que la otra aplicación ha enviado.\n",
    "\n",
    "Pero si intenta leer un socket cuando el programa en el otro extremo del socket no ha enviado ningún dato, simplemente siéntese y espere. Si los programas en ambos extremos del socket simplemente esperan algunos datos sin enviar nada, esperarán durante mucho tiempo.\n",
    "\n",
    "Entonces, una parte importante de los programas que se comunican a través de Internet es tener algún tipo de protocolo. Un protocolo es un conjunto de reglas precisas que determinan quién debe ir primero, qué deben hacer y, a continuación, cuáles son las respuestas a ese mensaje y quién envía a continuación, y así sucesivamente. En cierto sentido, las dos aplicaciones en cada extremo del socket están haciendo un baile y asegurándose de no pisar los dedos del otro.\n",
    "\n",
    "Hay muchos documentos que describen estos protocolos de red. El protocolo de transferencia de hipertexto se describe en el siguiente [documento](https://www.w3.org/Protocols/rfc2616/rfc2616.txt).\n",
    "\n",
    "Este es un documento largo y complejo de 176 páginas con muchos detalles. Si le parece interesante, siéntase libre de leerlo todo. Pero si echas un vistazo a la página 36 de RFC2616, encontrarás la sintaxis para la solicitud GET. Para solicitar un documento de un servidor web, hacemos una conexión con el servidor `www.pr4e.org` en el puerto 80 y luego enviamos una línea del formulario.\n",
    "\n",
    "    GET http://data.pr4e.org/romeo.txt HTTP/1.0\n",
    "\n",
    "donde el segundo parámetro es la página web que estamos solicitando, y luego también enviamos una línea en blanco. El servidor web responderá con cierta información de encabezado sobre el documento y una línea en blanco seguida del contenido del documento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El navegador web mas simple del mundo\n",
    "\n",
    "Quizás la forma más fácil de mostrar cómo funciona el protocolo HTTP es escribir un programa Python muy simple que hace una conexión a un servidor web y sigue las reglas del protocolo HTTP para solicitar un documento y mostrar lo que el servidor envía de vuelta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\r\n",
      "Date: Sat, 18 Jan 2020 19:22:11 GMT\r\n",
      "Server: Apache/2.4.18 (Ubuntu)\r\n",
      "Last-Modified: Sat, 13 May 2017 11:22:22 GMT\r\n",
      "ETag: \"a7-54f6609245537\"\r\n",
      "Accept-Ranges: bytes\r\n",
      "Content-Length: 167\r\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\r\n",
      "Pragma: no-cache\r\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\r\n",
      "Connection: close\r\n",
      "Content-Type: text/plain\r\n",
      "\r\n",
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "misocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "misocket.connect(('data.pr4e.org', 80))\n",
    "cmd = 'GET http://data.pr4e.org/romeo.txt HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "misocket.send(cmd)\n",
    "\n",
    "while True:\n",
    "    datos = misocket.recv(512)\n",
    "    if len(datos) < 1:\n",
    "        break\n",
    "    print(datos.decode(),end='')\n",
    "\n",
    "misocket.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, el programa establece una conexión con el puerto 80 en el servidor `www.py4e.com` . Como nuestro programa cumple la función de \"navegador web\", el protocolo HTTP dice que debemos enviar el comando GET seguido de una línea en blanco.\n",
    "\n",
    "![Figura 12.1](./figures/12.1.svg)\n",
    "<center><i>Figura 12.1: Una conexión de socket</i></center>\n",
    "\n",
    "Una vez que enviamos esa línea en blanco, escribimos un bucle que recibe datos en trozos de 512 caracteres del socket e imprime los datos hasta que no hay más datos para leer (es decir, el `recv()` devuelve una cadena vacía).\n",
    "\n",
    "La salida comienza con los encabezados que envía el servidor web para describir el documento. Por ejemplo, el encabezado `Content-Type` indica que el documento es un documento de texto plano (text/plain).\n",
    "\n",
    "Después de que el servidor nos envíe los encabezados, agrega una línea en blanco para indicar el final de los encabezados y luego envía los datos reales del archivo `romeo.txt`.\n",
    "\n",
    "Este ejemplo muestra cómo hacer una conexión de red de bajo nivel con sockets. Los sockets se pueden usar para comunicarse con un servidor web o con un servidor de correo u otros tipos de servidores. Todo lo que se necesita es encontrar el documento que describe el protocolo y escribir el código para enviar y recibir los datos de acuerdo con el protocolo.\n",
    "\n",
    "Sin embargo, dado que el protocolo que utilizamos más comúnmente es el protocolo web HTTP, Python tiene una biblioteca especial diseñada específicamente para admitir el protocolo HTTP para la recuperación de documentos y datos en la web.\n",
    "\n",
    "Uno de los requisitos para usar el protocolo HTTP es la necesidad de enviar y recibir datos como objetos de bytes, en lugar de cadenas. En el ejemplo anterior, los métodos `encode()` y `decode()` convierten cadenas en objetos de bytes y viceversa.\n",
    "\n",
    "El siguiente ejemplo usa notación `b''` para especificar que una variable debe almacenarse como un objeto de bytes. `encode()` y `b''` son equivalentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello world'\n",
      "b'Hello world'\n"
     ]
    }
   ],
   "source": [
    "print(b'Hello world')\n",
    "print('Hello world'.encode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recuperando una imagen con HTTP\n",
    "\n",
    "En el ejemplo anterior, recuperamos un archivo de texto sin formato que tenía nuevas líneas en el archivo y simplemente copiamos los datos en la pantalla mientras se ejecutaba el programa. Podemos usar un programa similar para recuperar una imagen usando HTTP. En lugar de copiar los datos en la pantalla mientras se ejecuta el programa, acumulamos los datos en una cadena, recortamos los encabezados y luego guardamos los datos de la imagen en un archivo de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5120 5120\n",
      "1980 7100\n",
      "5120 12220\n",
      "1980 14200\n",
      "5120 19320\n",
      "560 19880\n",
      "5120 25000\n",
      "560 25560\n",
      "5120 30680\n",
      "4820 35500\n",
      "2840 38340\n",
      "2840 41180\n",
      "5120 46300\n",
      "1980 48280\n",
      "4260 52540\n",
      "5120 57660\n",
      "3400 61060\n",
      "5120 66180\n",
      "3400 69580\n",
      "1420 71000\n",
      "5120 76120\n",
      "1980 78100\n",
      "2840 80940\n",
      "2840 83780\n",
      "1420 85200\n",
      "2840 88040\n",
      "4260 92300\n",
      "5120 97420\n",
      "1980 99400\n",
      "5120 104520\n",
      "4820 109340\n",
      "1420 110760\n",
      "5120 115880\n",
      "560 116440\n",
      "5120 121560\n",
      "4820 126380\n",
      "4260 130640\n",
      "5120 135760\n",
      "5120 140880\n",
      "1120 142000\n",
      "5120 147120\n",
      "3400 150520\n",
      "4260 154780\n",
      "2840 157620\n",
      "1420 159040\n",
      "5120 164160\n",
      "1800 165960\n",
      "5120 171080\n",
      "5120 176200\n",
      "3960 180160\n",
      "5120 185280\n",
      "3400 188680\n",
      "2840 191520\n",
      "4440 195960\n",
      "5120 201080\n",
      "5120 206200\n",
      "5120 211320\n",
      "5120 216440\n",
      "3660 220100\n",
      "1420 221520\n",
      "1420 222940\n",
      "1420 224360\n",
      "1420 225780\n",
      "4260 230040\n",
      "568 230608\n",
      "Header length 394\n",
      "HTTP/1.1 200 OK\n",
      "Date: Sat, 18 Jan 2020 19:22:11 GMT\n",
      "Server: Apache/2.4.18 (Ubuntu)\n",
      "Last-Modified: Mon, 15 May 2017 12:27:40 GMT\n",
      "ETag: \"38342-54f8f2e5b6277\"\n",
      "Accept-Ranges: bytes\n",
      "Content-Length: 230210\n",
      "Vary: Accept-Encoding\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\n",
      "Pragma: no-cache\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\n",
      "Connection: close\n",
      "Content-Type: image/jpeg\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import time\n",
    "\n",
    "HOST = 'data.pr4e.org'\n",
    "PUERTO = 80\n",
    "misocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "misocket.connect((HOST, PUERTO))\n",
    "misocket.sendall(b'GET http://data.pr4e.org/cover3.jpg HTTP/1.0\\r\\n\\r\\n')\n",
    "count = 0\n",
    "imagen = b\"\"\n",
    "\n",
    "while True:\n",
    "    datos = misocket.recv(5120)\n",
    "    if len(datos) < 1: break\n",
    "    #time.sleep(0.25)\n",
    "    count = count + len(datos)\n",
    "    print(len(datos), count)\n",
    "    imagen = imagen + datos\n",
    "\n",
    "misocket.close()\n",
    "\n",
    "# Look for the end of the header (2 CRLF)\n",
    "pos = imagen.find(b\"\\r\\n\\r\\n\")\n",
    "print('Header length', pos)\n",
    "print(imagen[:pos].decode())\n",
    "\n",
    "# Skip past the header and save the picture data\n",
    "imagen = imagen[pos+4:]\n",
    "fhand = open(\"stuff.jpg\", \"wb\")\n",
    "fhand.write(imagen)\n",
    "fhand.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede ver que para esta url, el encabezado Content-Type indica que el cuerpo del documento es una imagen (image/jpeg). Una vez que se completa el programa, puede ver los datos de la imagen abriendo el archivo stuff.jpg en un visor de imágenes.\n",
    "\n",
    "A medida que el programa se ejecuta, puede ver que no obtenemos 5120 caracteres cada vez que llamamos al método `recv()`. Obtenemos todos los caracteres que el servidor web nos transfirió a través de la red en el momento en que llamamos `recv()`. En este ejemplo, obtenemos 1460 o 2920 caracteres cada vez que solicitamos hasta 5120 caracteres de datos.\n",
    "\n",
    "Sus resultados pueden ser diferentes dependiendo de la velocidad de su red. También tenga en cuenta que en la última llamada a `recv()` recibimos 1681 bytes, que es el final de la secuencia, y en la siguiente llamada `recv()` obtenemos una cadena de longitud cero que nos dice que el servidor ha llamado a `close()` en su extremo del socket y allí ya no hay más información próxima.\n",
    "\n",
    "Podemos ralentizar nuestras llamadas `recv()` sucesivas al quitarle el comentario a la llamada `time.sleep()`. De esta manera, esperamos un cuarto de segundo después de cada llamada para que el servidor pueda \"adelantarse\" y enviarnos más datos antes de volver a llamar `recv()`. Con el retraso, el programa se ejecuta de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5120 5120\n",
      "5120 10240\n",
      "5120 15360\n",
      "5120 20480\n",
      "5120 25600\n",
      "5120 30720\n",
      "5120 35840\n",
      "5120 40960\n",
      "5120 46080\n",
      "5120 51200\n",
      "5120 56320\n",
      "5120 61440\n",
      "5120 66560\n",
      "5120 71680\n",
      "5120 76800\n",
      "5120 81920\n",
      "5120 87040\n",
      "5120 92160\n",
      "5120 97280\n",
      "5120 102400\n",
      "5120 107520\n",
      "5120 112640\n",
      "5120 117760\n",
      "5120 122880\n",
      "5120 128000\n",
      "5120 133120\n",
      "5120 138240\n",
      "5120 143360\n",
      "5120 148480\n",
      "5120 153600\n",
      "5120 158720\n",
      "5120 163840\n",
      "5120 168960\n",
      "5120 174080\n",
      "5120 179200\n",
      "5120 184320\n",
      "5120 189440\n",
      "5120 194560\n",
      "5120 199680\n",
      "5120 204800\n",
      "5120 209920\n",
      "5120 215040\n",
      "5120 220160\n",
      "5120 225280\n",
      "5120 230400\n",
      "208 230608\n",
      "Header length 394\n",
      "HTTP/1.1 200 OK\n",
      "Date: Sat, 18 Jan 2020 19:22:12 GMT\n",
      "Server: Apache/2.4.18 (Ubuntu)\n",
      "Last-Modified: Mon, 15 May 2017 12:27:40 GMT\n",
      "ETag: \"38342-54f8f2e5b6277\"\n",
      "Accept-Ranges: bytes\n",
      "Content-Length: 230210\n",
      "Vary: Accept-Encoding\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\n",
      "Pragma: no-cache\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\n",
      "Connection: close\n",
      "Content-Type: image/jpeg\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import time\n",
    "\n",
    "HOST = 'data.pr4e.org'\n",
    "PUERTO = 80\n",
    "misocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "misocket.connect((HOST, PUERTO))\n",
    "misocket.sendall(b'GET http://data.pr4e.org/cover3.jpg HTTP/1.0\\r\\n\\r\\n')\n",
    "count = 0\n",
    "imagen = b\"\"\n",
    "\n",
    "while True:\n",
    "    datos = misocket.recv(5120)\n",
    "    if len(datos) < 1: break\n",
    "    time.sleep(0.25)\n",
    "    count = count + len(datos)\n",
    "    print(len(datos), count)\n",
    "    imagen = imagen + datos\n",
    "\n",
    "misocket.close()\n",
    "\n",
    "# Look for the end of the header (2 CRLF)\n",
    "pos = imagen.find(b\"\\r\\n\\r\\n\")\n",
    "print('Header length', pos)\n",
    "print(imagen[:pos].decode())\n",
    "\n",
    "# Skip past the header and save the picture data\n",
    "imagen = imagen[pos+4:]\n",
    "fhand = open(\"stuff.jpg\", \"wb\")\n",
    "fhand.write(imagen)\n",
    "fhand.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, aparte de la última llamada a `recv()`, ahora obtenemos 5120 caracteres cada vez que solicitamos nuevos datos.\n",
    "\n",
    "Hay un búfer entre las solicitudes `send()` de realización del servidor y nuestras solicitudes de realización de aplicaciones `recv()`. Cuando ejecutamos el programa con la demora en su lugar, en algún momento el servidor puede llenar el búfer en el socket y verse obligado a pausar hasta que nuestro programa comience a vaciar el búfer. La pausa de la aplicación de envío o la aplicación de recepción se denomina \"control de flujo\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recuperando páginas web con `urllib`\n",
    "\n",
    "Si bien podemos enviar y recibir datos manualmente a través de HTTP utilizando la biblioteca de socket, hay una manera mucho más simple de realizar esta tarea común en Python al usar la biblioteca `urllib`.\n",
    "\n",
    "Usando `urllib`, puedes tratar una página web como un archivo. Simplemente indica qué página web desea recuperar y `urllib` maneja todos los detalles del encabezado y el protocolo HTTP.\n",
    "\n",
    "El código equivalente para leer el archivo `romeo.txt` desde la web con `urllib` es el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "fhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "for linea in fhand:\n",
    "    print(linea.decode().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que se ha abierto la página web usando `urllib.urlopen`, podemos tratarla como un archivo y leerla mediante un bucle `for`.\n",
    "\n",
    "Cuando se ejecuta el programa, solo vemos el resultado del contenido del archivo. Los encabezados aún se envían, pero el código `urllib` suprime los encabezados y solo nos devuelve los datos.\n",
    "\n",
    "Como ejemplo, podemos escribir un programa para recuperar los datos `romeo.txt` y calcular la frecuencia de cada palabra en el archivo de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'But': 1, 'soft': 1, 'what': 1, 'light': 1, 'through': 1, 'yonder': 1, 'window': 1, 'breaks': 1, 'It': 1, 'is': 3, 'the': 3, 'east': 1, 'and': 3, 'Juliet': 1, 'sun': 2, 'Arise': 1, 'fair': 1, 'kill': 1, 'envious': 1, 'moon': 1, 'Who': 1, 'already': 1, 'sick': 1, 'pale': 1, 'with': 1, 'grief': 1}\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "fhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "counts = dict()\n",
    "for linea in fhand:\n",
    "    palabras = linea.decode().split()\n",
    "    for palabra in palabras:\n",
    "        counts[palabra] = counts.get(palabra, 0) + 1\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente, una vez que hemos abierto la página web, podemos leerla como un archivo local."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de archivos binarios usando `urllib`\n",
    "\n",
    "En ocasiones, desea recuperar un archivo que no sea de texto (o binario), como un archivo de imagen o video. Los datos en estos archivos generalmente no son útiles para imprimir, pero puede hacer fácilmente una copia de una URL a un archivo local en su disco duro usando `urllib`.\n",
    "\n",
    "El patrón es abrir la URL y usarla `read` para descargar todo el contenido del documento en una variable de cadena (img) y luego escribir esa información en un archivo local de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "img = urllib.request.urlopen('http://data.pr4e.org/cover3.jpg').read()\n",
    "fhand = open('cover3.jpg', 'wb')\n",
    "fhand.write(img)\n",
    "fhand.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este programa lee todos los datos a la vez a través de la red y los almacena en la variable `img` en la memoria principal de su computadora, luego abre el archivo `cover3.jpg` y escribe los datos en su disco. El argumento `wb` para open() abre un archivo binario para escribir solo. Este programa funcionará si el tamaño del archivo es menor que el tamaño de la memoria de su computadora.\n",
    "\n",
    "Sin embargo, si se trata de un archivo de audio o video de gran tamaño, este programa puede bloquearse o al menos ejecutarse extremadamente lento cuando la computadora se queda sin memoria. Para evitar quedarse sin memoria, recuperamos los datos en bloques (o buffers) y luego escribimos cada bloque en su disco antes de recuperar el siguiente bloque. De esta manera, el programa puede leer archivos de cualquier tamaño sin utilizar toda la memoria que tiene en su computadora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230210 carácteres coiados.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "img = urllib.request.urlopen('http://data.pr4e.org/cover3.jpg')\n",
    "fhand = open('cover3.jpg', 'wb')\n",
    "size = 0\n",
    "while True:\n",
    "    info = img.read(100000)\n",
    "    if len(info) < 1: break\n",
    "    size = size + len(info)\n",
    "    fhand.write(info)\n",
    "\n",
    "print(size, 'carácteres coiados.')\n",
    "fhand.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, leemos solo 100,000 caracteres a la vez y luego los escribimos en el archivo `cover.jpg` antes de recuperar los siguientes 100,000 caracteres de datos de la web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analizando HTML y raspando la web\n",
    "\n",
    "Uno de los usos comunes de `urllib` en Python es la capacidad de \"raspar la web\" también conocido como \"web scraping\". El raspado web es cuando escribimos un programa que pretende ser un buscador web y recupera páginas, luego examina los datos en esas páginas\n",
    "buscando patrones.\n",
    "\n",
    "Como ejemplo, un motor de búsqueda como Google buscará en el origen de una página web y extraerá los enlaces a otras páginas y recuperará esas páginas, extraerá enlaces, y así sucesivamente. Con esta técnica, Google navega a través de casi todas las páginas de la web.\n",
    "\n",
    "Google también usa la frecuencia de los enlaces de las páginas que encuentra en una página en particular como una medida de cuán \"importante\" es una página y qué tan alta debe aparecer la página en sus resultados de búsqueda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de HTML utilizando expresiones regulares\n",
    "\n",
    "Una forma simple de analizar HTML es usar expresiones regulares para buscar y extraer subcadenas que coincidan con un patrón en particular. Aquí hay una página web simple:\n",
    "\n",
    "    <h1>The First Page</h1>\n",
    "    <p>\n",
    "    If you like, you can switch to the\n",
    "    <a href=\"http://www.dr-chuck.com/page2.htm\">\n",
    "    Second Page</a>.\n",
    "    </p>\n",
    "\n",
    "Podemos construir una expresión regular bien formada para unir y extraer los valores del enlace del texto anterior de la siguiente manera:\n",
    "\n",
    "    href=\"http://.+?\"\n",
    "    \n",
    "Nuestra expresión regular busca cadenas que comiencen con `href=\"http://`, seguido de uno o más caracteres `.+?`, Seguido de otra comilla doble. El signo de interrogación agregado a `\". +? \"` indica que el partido se debe hacer de una manera \"no codiciosa\" en lugar de \"codiciosa\". Un partido no codicioso intenta encontrar la secuencia de combinación más pequeña posible y un partido codicioso intenta encontrar la secuencia de mayor coincidencia posible.\n",
    "\n",
    "Agregamos paréntesis a nuestra expresión regular para indicar qué parte de nuestra cadena coincidente nos gustaría extraer y generar el siguiente programa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter - https://docs.python.org\n",
      "http://sphinx.pocoo.org/\n"
     ]
    }
   ],
   "source": [
    "# Busqueda de lineas que inicien con From y tengan un @\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import re\n",
    "\n",
    "url = input('Enter - ')\n",
    "html = urllib.request.urlopen(url).read()\n",
    "links = re.findall(b'href=\"(http://.*?)\"', html)\n",
    "for link in links:\n",
    "    print(link.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las expresiones regulares funcionan muy bien cuando su HTML está bien formateado y es predecible. Pero dado que hay muchas páginas HTML \"rotas\", una solución que solo utiliza expresiones regulares puede omitir algunos enlaces válidos o terminar con datos incorrectos.\n",
    "\n",
    "Esto se puede resolver utilizando una robusta biblioteca de análisis HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de HTML usando BeautifulSoup\n",
    "\n",
    "Hay varias bibliotecas de Python que pueden ayudarlo a analizar HTML y extraer datos de las páginas. Cada una de las bibliotecas tiene sus fortalezas y debilidades y puede elegir una según sus necesidades.\n",
    "\n",
    "Como ejemplo, simplemente analizaremos algunos enlaces de entrada y extracción de HTML utilizando la\n",
    "biblioteca `BeautifulSoup`. Puede descargar e instalar el código BeautifulSoup desde: http://www.crummy.com/software/.\n",
    "\n",
    "Puede descargar e \"instalar\" `BeautifulSoup` o simplemente puede colocar el archivo BeautifulSoup.py en la misma carpeta que su aplicación.\n",
    "\n",
    "Aunque el HTML se parece a XML y algunas páginas están cuidadosamente construidas para ser XML, la mayoría de los HTML se suelen romper de tal forma que hacen que un analizador XML rechace la página completa de HTML como inadecuada. `BeautifulSoup` tolera HTML altamente defectuoso y aún le permite extraer fácilmente los datos que necesita.\n",
    "\n",
    "Usaremos `urllib` para leer la página y luego usaremos `BeautifulSoup` para extraer los atributos `href` de las etiquetas de anclaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingresa un url - https://docs.python.org\n",
      "genindex.html\n",
      "py-modindex.html\n",
      "https://www.python.org/\n",
      "#\n",
      "whatsnew/3.8.html\n",
      "whatsnew/index.html\n",
      "tutorial/index.html\n",
      "library/index.html\n",
      "reference/index.html\n",
      "using/index.html\n",
      "howto/index.html\n",
      "installing/index.html\n",
      "distributing/index.html\n",
      "extending/index.html\n",
      "c-api/index.html\n",
      "faq/index.html\n",
      "py-modindex.html\n",
      "genindex.html\n",
      "glossary.html\n",
      "search.html\n",
      "contents.html\n",
      "bugs.html\n",
      "https://devguide.python.org/docquality/#helping-with-documentation\n",
      "about.html\n",
      "license.html\n",
      "copyright.html\n",
      "download.html\n",
      "https://docs.python.org/3.9/\n",
      "https://docs.python.org/3.8/\n",
      "https://docs.python.org/3.7/\n",
      "https://docs.python.org/3.6/\n",
      "https://docs.python.org/3.5/\n",
      "https://docs.python.org/2.7/\n",
      "https://www.python.org/doc/versions/\n",
      "https://www.python.org/dev/peps/\n",
      "https://wiki.python.org/moin/BeginnersGuide\n",
      "https://wiki.python.org/moin/PythonBooks\n",
      "https://www.python.org/doc/av/\n",
      "https://devguide.python.org/\n",
      "genindex.html\n",
      "py-modindex.html\n",
      "https://www.python.org/\n",
      "#\n",
      "copyright.html\n",
      "https://www.python.org/psf/donations/\n",
      "https://docs.python.org/3/bugs.html\n",
      "http://sphinx.pocoo.org/\n"
     ]
    }
   ],
   "source": [
    "# Para ejecutar esto necesitas intallar BeautifulSoup\n",
    "# https://pypi.python.org/pypi/beautifulsoup4\n",
    "\n",
    "# o descargar el archivo\n",
    "# http://www.py4e.com/code3/bs4.zip\n",
    "# y descomprimirlo en el mismo directorio de este archivo\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignora errores de certificados SSL\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Ingresa un url - ')\n",
    "html = urllib.request.urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    print(tag.get('href', None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El programa solicita una dirección web, luego abre la página web, lee los datos y los pasa al analizador `BeautifulSoup`, y luego recupera todas las etiquetas de anclaje e imprime el atributo `href` para cada etiqueta.\n",
    "\n",
    "Esta lista es mucho más larga porque algunas etiquetas de anclaje HTML son rutas relativas (por ejemplo, tutorial/index.html) o referencias en la página (por ejemplo, '#') que no incluyen \"http://\" o \"https://\", que era un requisito en nuestra expresión regular.\n",
    "\n",
    "También puede usar `BeautifulSoup` para extraer varias partes de cada etiqueta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingresa un url - http://www.dr-chuck.com/page1.htm\n",
      "TAG: <a href=\"http://www.dr-chuck.com/page2.htm\">\n",
      "Second Page</a>\n",
      "URL: http://www.dr-chuck.com/page2.htm\n",
      "Contents: \n",
      "Second Page\n",
      "Attrs: {'href': 'http://www.dr-chuck.com/page2.htm'}\n"
     ]
    }
   ],
   "source": [
    "# Para ejecutar esto necesitas intallar BeautifulSoup\n",
    "# https://pypi.python.org/pypi/beautifulsoup4\n",
    "\n",
    "# o descargar el archivo\n",
    "# http://www.py4e.com/code3/bs4.zip\n",
    "# y descomprimirlo en el mismo directorio de este archivo\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignora errores de certificados SSL\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Ingresa un url - ')\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Recupera todas las etiquetas de anclaje\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    # Look at the parts of a tag\n",
    "    print('TAG:', tag)\n",
    "    print('URL:', tag.get('href', None))\n",
    "    print('Contents:', tag.contents[0])\n",
    "    print('Attrs:', tag.attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`html.parser` es el analizador HTML incluido en la biblioteca estándar de Python 3. La información sobre otros analizadores HTML está disponible en: http://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser. Estos ejemplos solo comienzan a mostrar el poder de `BeautifulSoup` cuando se trata de analizar HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comandos `curl`\n",
    "\n",
    "Si tiene una computadora Linux, Unix, Macintosh, o Windows 10 es probable que tenga comandos integrados en su sistema operativo que recupere tanto texto sin formato como archivos binarios utilizando los protocolos HTTP o File Transfer (FTP). Uno de estos comandos es `curl`:\n",
    "\n",
    "    $ curl -O http://www.py4e.com/cover.jpg\n",
    "\n",
    "El comando `curl` es la abreviatura de \"copiar URL\" y, por lo tanto, los dos ejemplos enumerados anteriormente para recuperar archivos binarios con urllib tienen un nombre inteligente `curl1.py` y `curl2.py`, ya que implementan una funcionalidad similar al comando `curl`. También hay un programa de muestra llamado `curl3.py` que realiza esta tarea de manera un poco más efectiva, en caso de que realmente quiera usar este patrón en un programa que está escribiendo.\n",
    "\n",
    "Un segundo comando que funciona de manera muy similar es `wget` (no esta disponible en Windows):\n",
    "\n",
    "    $ wget http://www.py4e.com/cover.jpg\n",
    "    \n",
    "Ambos comandos hacen que la recuperación de páginas web y archivos remotos sea una tarea simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glosario\n",
    "\n",
    "* **BeautifulSoup:** Una biblioteca de Python para analizar documentos HTML y extraer datos de documentos HTML que compensa la mayoría de las imperfecciones en el HTML que los navegadores generalmente ignoran. Puede descargar el código de BeautifulSoup desde www.crummy.com .\n",
    "* **puerto:** Un número que generalmente indica con qué aplicación se está contactando cuando realiza una conexión de socket a un servidor. Como ejemplo, el tráfico web generalmente usa el puerto 80 mientras que el tráfico de correo electrónico usa el puerto 25.\n",
    "* **raspar:** Cuando un programa pretende ser un navegador web y recupera una página web, luego mira el contenido de la página web. A menudo, los programas siguen los enlaces en una página para encontrar la página siguiente y poder atravesar una red de páginas o una red social.\n",
    "* **socket:** Una conexión de red entre dos aplicaciones donde las aplicaciones pueden enviar y recibir datos en cualquier dirección.\n",
    "* **araña:** El acto de un motor de búsqueda web recuperando una página y luego todas las páginas enlazadas desde una página y así sucesivamente hasta que tengan casi todas las páginas en Internet que usan para construir su índice de búsqueda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "**Ejercicio 1:** cambie el programa de socket `socket1.py` para solicitar al usuario la URL para que pueda leer cualquier página web. Puede usar `split('/')` para dividir la URL en partes para poder extraer el nombre de host para la llamada del socket `connect`. Agregue la comprobación de errores usando `try` y `except` para manejar la condición en la que el usuario ingresa una URL con formato incorrecto o inexistente.\n",
    "\n",
    "**Ejercicio 2:** cambie su programa de socket para que cuente el número de caracteres que ha recibido y deje de mostrar cualquier texto después de que haya mostrado 3000 caracteres. El programa debe recuperar el documento completo y contar el número total de caracteres y mostrar el recuento del número de caracteres al final del documento.\n",
    "\n",
    "**Ejercicio 3:** se utiliza `urllib` para replicar el ejercicio anterior de (1) recuperar el documento de una URL, (2) mostrar hasta 3000 caracteres y (3) contar el número total de caracteres en el documento. No se preocupe por los encabezados de este ejercicio, simplemente muestre los primeros 3000 caracteres del contenido del documento.\n",
    "\n",
    "**Ejercicio 4:** cambie el programa `urllinks.py` para extraer y contar etiquetas de párrafo (p) del documento HTML recuperado y mostrar el recuento de los párrafos como la salida de su programa. No debes mostrar el texto del párrafo, solo contarlos. Pruebe su programa en varias páginas web pequeñas, así como en algunas páginas web más grandes.\n",
    "\n",
    "**Ejercicio 5:** (Avanzado) Cambie el programa de socket para que solo muestre datos después de que se hayan recibido los encabezados y una línea en blanco. Recuerde que `recv` recibe caracteres (líneas nuevas y todo), no líneas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "| [Indice](indice.ipynb) | \n",
    "\n",
    "< [Capítulo 11 - Expresiones regulares](cap11.ipynb) | [Capítulo 13 - Python y servicios web](cap13.ipynb) >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
